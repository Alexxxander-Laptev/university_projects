<HTML><HEAD>
<TITLE>МЕТОД ХАФФМАНА И РОДСТВЕННЫЕ МЕТОДЫ</TITLE>
<META content="text/html; charset=windows-1251" http-equiv="Content-Type">
<META NAME="description" CONTENT="Алгоритмы программирование, домашняя страничка Ворожцова Артема, Форумы, LaTeX MiKTeX TeX help описание, искусственный интеллект, язык знаний, архивы статей,">
<META NAME="keywords" CONTENT="Алгоритмы программирование, задание физтех, МФТИ, искусственный интеллект,  математика, Ворожцов Артём, автономные агенты, LaTeX, подробное описание, Поппер, Кант, словари, семантические сети, полные тексты,  Милан Кундера, PostScript, word2Tex">
<style>
A {text-decoration:none; font-weight:bold}
A:hover {color:"#a0303f"}
h1 {color:#603030;text-align=center; font-family:Arial,Helvetica}
h2 {color:#603030; text-align=center; font-style:italic;}
h3 {color:#50453A; margin-top:20pt; margin-bottom:0pt; }
p {margin-top:0}
p.left {text-align:left;}
p.right {text-align:right;}
p.center {text-align:center; margin-top:0; margin-bottom:0}
table.small {font-size:12pt}
table.sample {font-weight:bold; color:#50453A;}
pre {color:navy; font-weight:bold}
pre.long {color:#50453A; font-weight:bold}
span.code {color:#50453A; font-weight:bold}

table.sm {font-weight:bold; color:#50453A; margin-top:0; margin-left:0; style:"bgcolor=blue"}
span.rem {color: #404040; font-style:italic}
span.formula {color: #401010}
span.com {color: #101040}
span.key {color: #104010; font-style:italic}
span.sec {color: #901010; font-style:italic}
span.be {color: #005010; font-style:italic}
span.br {color: #505010;}
span.bibl {color: #104040;}
img {margin-left:0; margin-right:0;}
span.key {font-weight: bold; color: navy}
span.op { color: blue}
span.dir {color: green}
li {margin-top:8}
</style>
</head>
<body bgcolor="#706050" link="#30308f" vlink="#30308f" alink="#30308f" text="black">
<A name="top"></A>
<div align=center>
<table width=700 border=1 cellspacing=3 cellpadding=16 bgcolor="f0f0e0">
<tr><td valign=top>
<table border=0 width=700><tr>
<td valign=top>
<a href="http://www.dgap.mipt.ru/~artema/index.html"><img align=left border=0 src=logob000.gif></a>
<a href="http://mindspring.narod.ru/alg/rcompress.html">[Обзор&nbsp;методов&nbsp;сжатия&nbsp;данных]</a> &nbsp; &nbsp; <a href="">[Huffman]</a> &nbsp; &nbsp;
<a href="http://mindspring.narod.ru/alg/rastr.html">[Растровая&nbsp;графика:взгляд&nbsp;внутрь]</a> &nbsp; &nbsp;

<a href="http://alglib.chat.ru/">[Библиотека&nbsp;алгоритмов]</a>&nbsp;
<a href="http://mindspring.narod.ru/alg/kernigan.zip">[<i>Керниган&Ричи</i> Язык C]</a>&nbsp;
<a href="http://mindspring.narod.ru/alg/bogatyrev.zip">[<i>Богатырёв</i> Си в Unix]</a>&nbsp;
<a href="http://www.combinatorics.org/">[Journal&nbsp;of&nbsp;Combinatorics]</a>&nbsp;<a href="http://mindspring.narod.ru/math/pi.ps.zip">[Число&nbsp;Pi&nbsp;&#150;The&nbsp;quest&nbsp;for&nbsp;Pi (ps.zip)]</a>
<a href="http://catalog.aport.ru/rus/themes.asp?id=101&r=0">[Математика&nbsp;на&nbsp;Aport]</a>&nbsp;  
<a href="http://www.nature.ru/">[Научная&nbsp;сеть&nbsp;-NATURE.RU]</a>&nbsp; &nbsp;
<a href="http://mindspring.narod.ru/alg/tasks1.html">[Задачи 1]</a>
<a href="http://mindspring.narod.ru/alg/tasks2.html">[2]</a>
</td></tr></table>
<hr width="100%" align=center>


<H1>МЕТОД ХАФФМАНА И РОДСТВЕННЫЕ МЕТОДЫ</H1>

<BR><CENTER><H3><FONT COLOR="#A00000">Метод Хаффмана (Huffman method)</FONT></H3></CENTER>

<p ALIGN="justify">
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Сжатие Хаффмана - статистический метод сжатия, который 
уменьшает среднюю длину кодового слова для символов алфавита. Код Хаффмана является примером кода, 
оптимального в случае, когда все вероятности появления символов в сообщении - целые отрицательные 
степени двойки. Код Хаффмана может быть построен по следующему алгоритму:<BR>
</P>

<OL>
<LI><p ALIGN="justify">Выписываем в ряд все символы алфавита в порядке 
возрастания или убывания вероятности их появления в тексте;</P></LI>

<LI><p ALIGN="justify">
Последовательно объединяем два символа с наименьшими вероятностями появления в новый составной символ, 
вероятность появления которого полагается равной сумме вероятностей составляющих его символов; в конце концов мы 
построим дерево, каждый узел которого имеет суммарную вероятность всех узлов, находящихся ниже него;</P></LI>

<LI><p ALIGN="justify">
Прослеживаем путь к каждому листу дерева помечая направление к каждому узлу (например, направо - 1, налево - 0).
</P></LI>

</OL>

<p ALIGN="justify">
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Для заданного распределения частот символов может существовать несколько   
возможных кодов Хаффмана. Возможно определить 'каноническое' дерево Хаффмана, 
выбрав одно из возможных деревьев. Такое кононическое дерево может быть очень 
компактно, передавая только длину в битах для каждого кодового слова. Такой 
метод используется в большинстве архиваторов (pkzip, lha, zoo, arj, ...).<BR>
</P>





<!-- Метод Шеннона-Фано (Shannon-Fano method) -->

<BR><CENTER><H3><FONT COLOR="#A00000">Метод Шеннона-Фано (Shannon-Fano method)</FONT></H3></CENTER>

<p ALIGN="justify">
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Родственным методом для кодирования Хаффмана является кодирование Шеннона-Фано, 
которое осуществляется следующим образом:<BR></P>

<OL>
<LI><p ALIGN="justify">
Делим множество символов на два подмножества так, чтобы сумма вероятностей появления 
символов одного подмножества была примерно равна сумме вероятностей появления символов другого. 
Для левого подмножества каждому символу приписываем "0", для правого - "1".
</P></LI>

<LI>Повторяем шаг (1) до тех пор, пока все подмножества не будут состоять из одного элемента.</LI>
</OL>

<p ALIGN="justify">
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Алгоритм создания кода Хаффмана называется снизу-вверх, 
а Шеннона-Фано - сверху-вниз. Кодирование по Хаффману всегда дает оптимальные коды, по 
Шеннону-Фано иногда используется немного больше бит.<BR><BR><BR>




&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Алгортимы Хаффмана и Шеннона-Фано являются 
одними из классических, поэтому они часто используются в графических форматах. 
Они берут только частоту появления одинаковых байт в изображении и сопоставляют 
символам входного потока, которые встречаются большее число раз, цепочку бит меньшей 
длины. И напротив - встречающимся редко - цепочку большей длины. Для сбора статистики 
требуютдвух проходов по изображению. Коэффициенты сжатия алгоритмов : 1/8, 2/3, 1. Требуют записи в 
файл таблицы соответствия кодируемых символов и кодирующих цепочек. На практике используются их 
разновидности. Так, в некоторых случаях резонно либо использовать постоянную таблицу, либо 
строить ее адаптивно, т.е. в процессе архивации/разархивации. Эти приемы избавляют от двух 
проходовпо изображению и необходимости хранения таблицы вместе с файлом. Кодирование с фиксированной 
таблицей применяется в качестве последнего этапа архивациив JPEG.<BR>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Близкая модификация алгоритма используется при сжатии черно-белых 
изображений. Последовательности подряд идущих черных и белых точек заменяются числом, равным их количеству 
с признаком цвета. А этот ряд уже, в свою очередь, сжимается по Хаффману с фиксированной таблицей. 
Для того, чтобы понять использование этих двух методов для сжатия изображений, рекомендую обратиться 
к формату TIFF.<BR>
</P>





<!-- Арифметический метод (Arithmetic method) -->

<BR><CENTER><H3><FONT COLOR="#A00000">Арифметический метод (Arithmetic method)</FONT></H3></CENTER>

<p ALIGN="justify">
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Может показаться что кодирование Хаффмана или Шеннона-Фано лучшее средство 
для сжатия. Однако это не так. Как было замечено выше, эти методы оптимальны только в том случае, когда все 
символы в сообщении имеют вероятности появления равные целым отрицательным степеням двойки, что в общем случае не так.<BR>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Метод арифметического кодирования не имеет этого ограничения: он достигает 
одинакового эффекта, т.к. рассматривает сообщение как единое целое (что для кодирования по Хаффману потребовало 
бы нумерации каждого из всех возможных сообщений), и таким образом достигает теоретической энтропийной границы 
эффективности сжатия для любого источника.<BR>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Работа арифметического кодера состоит в представлении числа интервалом 
вещественных чисел от 0 до 1. По мере увеличения длины сообщения, интервал, необходимый для его представления, 
становится все меньше и меньше, а число бит, необходимых для задания этого интервала, увеличивается. Каждый 
символ собщения по порядку сокращает этот интервал пропорционально вероятности появления этого символа. Наиболее 
вероятный символ меньше всех сокращает интервал, и таким образом добавляет меньше бит к коду сообщения.<BR>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;В качестве примера арифметического кодирования рассмотрим такой пример: алфавит 
состоит из двух символов X и Y с вероятностями 0.66 и 0.33. Кодируя такое сообщение, посмотрим на первый символ: 
если это символ X, выбираем интервал (0; 2/3), если же это символ Y, то - (2/3; 1). Продолжая таким образом 
для двух символов, мы получим : XX - (0; 4/9), XY - (4/9; 6/9), YX - (6/9; 8/9) и для YY - (8/9; 1).<BR>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;В этом примере арифметический код не полностью эффективен, по причине 
сжатия короткого сообщения - на длинных сообщениях эффективность кодирования реально приближается к 100%.<BR>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Теперь когда у нас есть эффективный метод кодирования, что мы можем сделать 
с ним ? То что нам надо - метод построения модели данных, которую мы можем затем использовать вместе с кодером. 
Простейшей моделью, например, является фиксированная таблица вероятностей стандартных букв ангийского текста, которую
мы можем использовать для получения вероятностей букв. Улучшением этого метода является использование адаптивной 
модели, другими словами, модели, которая для сжатия последующих данных приспосабливает себя по уже сжатым данным. 
Мы можем преобразовать фиксированную модель в адаптивную изменяя вероятности символов после кодирования каждого 
нового символа. Однако, мы можем сделать больше, чем это.<BR>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Использование вероятностей символов по отдельности - не совсем хорошая оценка 
для энтропии данных: мы можем также использовать вероятности сочетаний символов. Лучшие из известных на сей момент 
компрессоры, использующие этот подход: DMC (Dynamic Markov Coding) начинает работу с марковской модели 0-порядка и 
постепенно расширяет начальную модель в процессе сжатия; PPM (Prediction by Partial Matching), ищет появления 
сжимаемого текста в контексте n-го порядка. Оба метода таким образом получают более лучшую модель сжимаемых данных, 
и в сочетании с арифметическим кодированием, приводят к лучшей степени сжатия.<BR>

<p>Итак, если кодеры, основанные на арифметическом кодировании, так хороши, почему 
они не используются широко? Не считая того, что они относительно новы и не успели войти в повсеместное использование, 
есть еще один существенный недостаток: они требуют гораздо больше вычислительных ресурсов, как ресурсов процессора, 
так и памяти. Построение сложных моделей для сжатия может не закончится из-за нехватки памяти (особенно в случае 
DMC, когда модель может неограниченно расти); и само арифметическое кодирование требует решения числовых задач 
большого объема.
</P>
</TD></TR></TABLE></div>
</BODY>
</HTML>
<!-- ><!-- "><!-- '><!-- --></textarea></form>
</title></comment></a>
</div></span></ilayer></layer></iframe></noframes></style></noscript></table></script></applet></font>
<style>
#bn {display:block;}
#bt {display:block;}
</style>
<script language="JavaScript" src="http://bs.yandex.ru/show/163?ycid=0%0A2%0A28%0A203%0A3281"></script>
<!-- mailto:spm111@yandex.ru -->
<!-- This document saved from http://mindspring.narod.ru/alg/huffman.html -->
